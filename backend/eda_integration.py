# -*- coding: utf-8 -*-
from __future__ import annotations

import warnings
warnings.filterwarnings("ignore")

from typing import Dict, List, Tuple, Any, Optional

import numpy as np
import pandas as pd

# Plotly (wizualizacje ‚Äì bez twardej zale≈ºno≈õci od Streamlit)
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

from scipy import stats

# ======================================================================
#  Fallbacki i importy opcjonalne
# ======================================================================

# smart_cache (fallback, je≈ºeli brak backend.cache_manager)
try:
    from backend.cache_manager import smart_cache  # type: ignore
except Exception:
    class _DummyCache:
        def cache_decorator(self, ttl: int = 3600):
            def deco(fn):
                def wrapper(*a, **k):
                    return fn(*a, **k)
                return wrapper
            return deco
    smart_cache = _DummyCache()  # type: ignore

# AI/LLM opisy (opcjonalnie)
try:
    from backend.ai_integration import AIDescriptionGenerator  # type: ignore
except Exception:
    AIDescriptionGenerator = None  # type: ignore

# SHAP / Permutation importance (opcjonalnie)
try:
    from backend.eda_shap_utils import (  # type: ignore
        compute_permutation_importance,
        compute_shap_importance,
    )
except Exception:
    def compute_permutation_importance(*args, **kwargs):
        return None
    def compute_shap_importance(*args, **kwargs):
        return None

# Streamlit ‚Äì u≈ºywamy defensywnie, tylko gdy jest dostƒôpny
try:
    import streamlit as st  # type: ignore
except Exception:
    st = None  # type: ignore


# ======================================================================
#  Klasa EDAAnalyzer
# ======================================================================

class EDAAnalyzer:
    """
    Eksploracyjna Analiza Danych (EDA) + auto-clean + raporty + wizualizacje.
    Wszystkie metody bezpieczne wzglƒôdem brak√≥w opcjonalnych zale≈ºno≈õci.
    """

    def __init__(self):
        self.analysis_cache: Dict[str, Any] = {}
        self.cache = smart_cache

    # ====================== AI / OPISY KOLUMN ======================

    @smart_cache.cache_decorator(ttl=3600)
    def generate_column_descriptions(self, df: pd.DataFrame) -> Dict[str, str]:
        """
        Generuje inteligentne opisy kolumn przy u≈ºyciu AI (je≈õli dostƒôpne)
        lub fallbacku statystycznego.
        """
        if AIDescriptionGenerator is not None:
            try:
                ai_gen = AIDescriptionGenerator()
                return ai_gen.generate_column_descriptions(df)
            except Exception:
                pass
        return self._generate_descriptions_fallback(df)

    def _generate_descriptions_fallback(self, df: pd.DataFrame) -> Dict[str, str]:
        """Symulacja opis√≥w kolumn bez AI ‚Äì na bazie statystyki + heurystyk."""
        descriptions: Dict[str, str] = {}

        for column in df.columns:
            col_data = df[column]
            dtype = col_data.dtype
            null_count = col_data.isnull().sum()
            null_pct = float((null_count / len(col_data)) * 100) if len(col_data) else 0.0
            unique_count = int(col_data.nunique(dropna=True))
            unique_pct = float((unique_count / len(col_data)) * 100) if len(col_data) else 0.0

            parts: List[str] = []

            if pd.api.types.is_numeric_dtype(col_data):
                # statystyki
                cc = col_data.dropna()
                if len(cc) > 0:
                    mean_val = float(cc.mean())
                    median_val = float(cc.median())
                    std_val = float(cc.std())
                    min_val = float(cc.min())
                    max_val = float(cc.max())
                else:
                    mean_val = median_val = std_val = min_val = max_val = np.nan

                if unique_count <= 10:
                    parts.append(f"Zmienna kategoryczna numeryczna ({unique_count} unikalnych)")
                elif self._is_id_column(column, col_data):
                    parts.append("Prawdopodobnie kolumna identyfikacyjna (ID)")
                elif self._is_year_like(column, col_data):
                    parts.append("Kolumna reprezentujƒÖca rok/daty")
                elif self._is_percentage(col_data):
                    parts.append("Warto≈õci najpewniej procentowe")
                elif self._is_binary_numeric(col_data):
                    parts.append("Zmienna binarna (0/1)")
                else:
                    parts.append("Zmienna numeryczna ciƒÖg≈Ça")

                if np.isfinite(min_val) and np.isfinite(max_val):
                    parts.append(f"Zakres: {min_val:.2f}‚Äì{max_val:.2f}")
                if np.isfinite(mean_val) and np.isfinite(median_val):
                    parts.append(f"≈örednia: {mean_val:.2f}, Mediana: {median_val:.2f}")

                if np.isfinite(std_val) and np.isfinite(mean_val) and mean_val != 0 and std_val > abs(mean_val):
                    parts.append("Wysoka zmienno≈õƒá")

            else:
                # tekst/kategoria/datetime-like
                most_common = col_data.value_counts(dropna=True).head(3)
                if self._is_name_column(column):
                    parts.append("Prawdopodobnie nazwy/etykiety tekstowe")
                elif unique_pct > 90:
                    parts.append("Kolumna o bardzo wysokiej unikalno≈õci (ID tekstowe?)")
                elif self._is_datetime_series(col_data):
                    parts.append("Dane czasowe (daty/czas)")
                elif unique_count <= 10:
                    parts.append(f"Zmienna kategoryczna ({unique_count} kategorii)")
                else:
                    parts.append(f"Zmienna tekstowa ({unique_count} unikalnych)")

                if len(most_common) > 0:
                    parts.append("Najczƒôstsze: " + ", ".join(map(str, most_common.index[:3])))

            if null_count > 0:
                parts.append(f"‚ö†Ô∏è {null_pct:.1f}% brak√≥w")

            if unique_count == 1:
                parts.append("‚ö†Ô∏è Wszystkie warto≈õci identyczne (bezu≈ºyteczna kolumna)")
            elif unique_pct < 1 and pd.api.types.is_numeric_dtype(col_data):
                parts.append("‚ö†Ô∏è Bardzo ma≈Ça r√≥≈ºnorodno≈õƒá warto≈õci")

            descriptions[column] = ". ".join(parts)

        return descriptions

    # ====================== AUTO-CLEAN ======================

    def auto_clean(
        self,
        df: pd.DataFrame,
        progress_cb: Optional[Any] = None,
        log_cb: Optional[Any] = None
    ) -> pd.DataFrame:
        """
        Pipeline czyszczenia:
        - daty (parsowanie),
        - duplikaty,
        - kolumny z >60% NaN,
        - kolumny sta≈Çe,
        - imputacja (num=mediana, kat=moda),
        - winsoryzacja IQR.
        """
        def p(step: float, msg: str):
            if progress_cb:
                try: progress_cb(step)
                except Exception: pass
            if log_cb:
                try: log_cb(msg)
                except Exception: pass

        p(0.02, "üöÄ Start auto-clean")
        dfc = df.copy()

        # 1) Daty
        date_like = [c for c in dfc.columns if any(k in c.lower() for k in ["date","czas","time","data","timestamp"])]
        for c in date_like:
            try:
                dfc[c] = pd.to_datetime(dfc[c], errors="ignore", utc=False, dayfirst=True)
            except Exception:
                pass
        p(0.10, f"üìÖ Daty przetworzone: {len(date_like)}")

        # 2) Duplikaty
        before = len(dfc)
        dfc = dfc.drop_duplicates()
        p(0.15, f"üóëÔ∏è Usuniƒôte duplikaty: {before - len(dfc)}")

        # 3) Kolumny z >60% NaN
        try:
            miss_ratio = dfc.isna().mean(numeric_only=False)
        except Exception:
            miss_ratio = dfc.isna().mean()
        drop_cols = [c for c, r in miss_ratio.items() if r >= 0.6]
        if drop_cols:
            dfc = dfc.drop(columns=drop_cols)
        p(0.25, f"‚ùå Kolumny >60% NaN: {len(drop_cols)}")

        # 4) Kolumny sta≈Çe
        nunique = dfc.nunique(dropna=False)
        const_cols = [c for c, u in nunique.items() if u <= 1]
        if const_cols:
            dfc = dfc.drop(columns=const_cols)
        p(0.33, f"üîí Sta≈Çe kolumny usuniƒôte: {len(const_cols)}")

        # 5) Imputacja
        num_cols = dfc.select_dtypes(include=["number"]).columns.tolist()
        cat_cols = [c for c in dfc.columns if c not in num_cols]

        for c in num_cols:
            if dfc[c].isna().any():
                dfc[c] = dfc[c].fillna(dfc[c].median())

        for c in cat_cols:
            if dfc[c].isna().any():
                try:
                    mode_val = dfc[c].mode(dropna=True).iloc[0]
                except Exception:
                    mode_val = "UNKNOWN"
                dfc[c] = dfc[c].fillna(mode_val)
        p(0.55, f"üîß Imputacja wykonana (num={len(num_cols)}, cat={len(cat_cols)})")

        # 6) Winsoryzacja IQR
        for c in num_cols:
            s = dfc[c]
            try:
                if s.notna().sum() > 20:
                    q1, q3 = s.quantile(0.25), s.quantile(0.75)
                    iqr = q3 - q1
                    if pd.notna(iqr) and iqr > 0:
                        lo, hi = q1 - 1.5 * iqr, q3 + 1.5 * iqr
                        dfc[c] = s.clip(lower=lo, upper=hi)
            except Exception:
                pass
        p(0.85, f"üìä Winsoryzacja IQR zako≈Ñczona")

        p(0.96, "‚úÖ Auto-clean gotowe")
        return dfc

    # ====================== RAPORT KOMPLEKSOWY ======================

    @smart_cache.cache_decorator(ttl=7200)
    def generate_comprehensive_eda_report(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Kompleksowy raport EDA (cache 2h)."""
        return {
            "basic_info": self._get_basic_info(df),
            "missing_data_analysis": self._analyze_missing_data(df),
            "numerical_analysis": self._analyze_numerical_columns(df),
            "categorical_analysis": self._analyze_categorical_columns(df),
            "correlation_analysis": self._analyze_correlations(df),
            "outlier_analysis": self._detect_outliers(df),
            "data_quality_issues": self._detect_data_quality_issues(df),
            "recommendations": self._generate_recommendations(df),
        }

    # --- Podmetody raportu ---

    def _get_basic_info(self, df: pd.DataFrame) -> Dict[str, Any]:
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        categorical_cols = df.select_dtypes(include=["object", "category"]).columns
        datetime_cols = df.select_dtypes(include=["datetime64[ns]", "datetime64[ns, UTC]"]).columns
        return {
            "shape": tuple(df.shape),
            "memory_usage_mb": float(df.memory_usage(deep=True).sum() / (1024 ** 2)),
            "column_types": {
                "numeric": int(len(numeric_cols)),
                "categorical": int(len(categorical_cols)),
                "datetime": int(len(datetime_cols)),
            },
            "duplicate_rows": int(df.duplicated().sum()),
            "total_missing_values": int(df.isnull().sum().sum()),
        }

    def _analyze_missing_data(self, df: pd.DataFrame) -> Dict[str, Any]:
        missing_data = df.isnull().sum()
        missing_pct = (missing_data / len(df)) * 100 if len(df) else 0.0
        missing_df = pd.DataFrame({
            "missing_count": missing_data,
            "missing_percentage": missing_pct
        }).sort_values("missing_percentage", ascending=False)

        patterns = self._find_missing_patterns(df)
        return {
            "missing_by_column": missing_df[missing_df["missing_count"] > 0].to_dict(),
            "columns_with_missing": missing_df[missing_df["missing_count"] > 0].index.tolist(),
            "missing_patterns": patterns,
            "total_complete_rows": int(len(df.dropna())),
            "rows_with_any_missing": int(len(df) - len(df.dropna())),
        }

    def _find_missing_patterns(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        patterns: List[Dict[str, Any]] = []
        if df.empty:
            return patterns
        miss = df.isnull()
        for c1 in miss.columns:
            for c2 in miss.columns:
                if c1 >= c2:
                    continue
                try:
                    corr = float(miss[c1].corr(miss[c2]))
                except Exception:
                    corr = np.nan
                if pd.notna(corr) and corr > 0.5:
                    patterns.append({
                        "columns": [c1, c2],
                        "correlation": corr,
                        "pattern_type": "correlated_missing",
                    })
        return patterns

    def _analyze_numerical_columns(self, df: pd.DataFrame) -> Dict[str, Any]:
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        out: Dict[str, Any] = {}
        for col in numeric_cols:
            s = df[col].dropna()
            if s.empty:
                continue
            try:
                d: Dict[str, Any] = {
                    "mean": float(s.mean()),
                    "median": float(s.median()),
                    "std": float(s.std()),
                    "min": float(s.min()),
                    "max": float(s.max()),
                    "range": float(s.max() - s.min()),
                    "skewness": float(stats.skew(s)),
                    "kurtosis": float(stats.kurtosis(s)),
                    "coefficient_of_variation": float(s.std() / s.mean()) if s.mean() != 0 else float("inf"),
                }
            except Exception:
                d = {k: None for k in [
                    "mean","median","std","min","max","range","skewness","kurtosis","coefficient_of_variation"
                ]}
            if len(s) > 8:
                try:
                    _, pval = stats.shapiro(s.sample(min(5000, len(s)), random_state=42))
                    d["normality_test_p_value"] = float(pval)
                    d["is_normal"] = bool(pval > 0.05)
                except Exception:
                    d["normality_test_p_value"] = None
                    d["is_normal"] = None
            out[col] = d
        return out

    def _analyze_categorical_columns(self, df: pd.DataFrame) -> Dict[str, Any]:
        cat_cols = df.select_dtypes(include=["object", "category"]).columns
        out: Dict[str, Any] = {}
        for col in cat_cols:
            s = df[col].dropna()
            if s.empty:
                continue
            vc = s.value_counts()
            d: Dict[str, Any] = {
                "unique_count": int(s.nunique()),
                "most_frequent": vc.index[0] if len(vc) > 0 else None,
                "most_frequent_count": int(vc.iloc[0]) if len(vc) > 0 else 0,
                "least_frequent": vc.index[-1] if len(vc) > 0 else None,
                "least_frequent_count": int(vc.iloc[-1]) if len(vc) > 0 else 0,
                "entropy": float(stats.entropy(vc.values)) if vc.sum() > 0 else 0.0,
                "top_10_categories": {str(k): int(v) for k, v in vc.head(10).to_dict().items()},
            }
            if len(vc) > 1 and vc.iloc[-1] != 0:
                d["imbalance_ratio"] = float(vc.iloc[0] / vc.iloc[-1])
                d["is_highly_imbalanced"] = bool(d["imbalance_ratio"] > 10)
            else:
                d["imbalance_ratio"] = None
                d["is_highly_imbalanced"] = None
            out[col] = d
        return out

    def _analyze_correlations(self, df: pd.DataFrame) -> Dict[str, Any]:
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) < 2:
            return {}
        corr = df[numeric_cols].corr()
        strong: List[Dict[str, Any]] = []
        for i in range(len(corr.columns)):
            for j in range(i + 1, len(corr.columns)):
                v = float(corr.iloc[i, j])
                if pd.notna(v) and abs(v) > 0.7:
                    strong.append({
                        "var1": corr.columns[i],
                        "var2": corr.columns[j],
                        "correlation": v,
                        "strength": "very_strong" if abs(v) > 0.9 else "strong",
                    })
        return {
            "correlation_matrix": {c: {r: float(v) for r, v in corr[c].items()} for c in corr.columns},
            "strong_correlations": strong,
            "multicollinearity_warning": len(strong) > 0,
        }

    def _detect_outliers(self, df: pd.DataFrame) -> Dict[str, Any]:
        out: Dict[str, Any] = {}
        for col in df.select_dtypes(include=[np.number]).columns:
            s = df[col].dropna()
            if s.empty:
                continue
            Q1 = s.quantile(0.25)
            Q3 = s.quantile(0.75)
            IQR = Q3 - Q1
            low, high = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR
            iqr_mask = (s < low) | (s > high)
            try:
                z = np.abs(stats.zscore(s, nan_policy="omit"))
                z_mask = pd.Series(z, index=s.index) > 3
            except Exception:
                z_mask = pd.Series(False, index=s.index)
            out[col] = {
                "iqr_outliers_count": int(iqr_mask.sum()),
                "iqr_outliers_percentage": float((iqr_mask.sum() / len(s)) * 100),
                "zscore_outliers_count": int(z_mask.sum()),
                "zscore_outliers_percentage": float((z_mask.sum() / len(s)) * 100),
                "outlier_bounds": {"lower": float(low), "upper": float(high)},
            }
        return out

    def _detect_data_quality_issues(self, df: pd.DataFrame) -> List[Dict[str, str]]:
        issues: List[Dict[str, str]] = []
        dup = int(df.duplicated().sum())
        if dup > 0:
            issues.append({
                "type": "duplicates",
                "severity": "medium",
                "description": f"Znaleziono {dup} zduplikowanych wierszy",
                "recommendation": "Rozwa≈º usuniƒôcie duplikat√≥w lub ich uzasadnienie",
            })
        # sta≈Çe kolumny
        for col in df.columns:
            try:
                if df[col].nunique(dropna=True) == 1:
                    issues.append({
                        "type": "constant_column",
                        "severity": "high",
                        "description": f"Kolumna '{col}' ma tylko jednƒÖ warto≈õƒá",
                        "recommendation": "Usu≈Ñ ‚Äì kolumna nie wnosi informacji",
                    })
            except Exception:
                pass
        # du≈ºo brak√≥w
        if len(df) > 0:
            miss_pct = (df.isnull().sum() / len(df)) * 100
            for col in miss_pct[miss_pct > 50].index.tolist():
                issues.append({
                    "type": "high_missing_data",
                    "severity": "high",
                    "description": f"Kolumna '{col}' ma {miss_pct[col]:.1f}% brak√≥w",
                    "recommendation": "Imputuj, uzupe≈Çnij u ≈∫r√≥d≈Ça lub usu≈Ñ kolumnƒô",
                })
        # problemy z encoding
        for col in df.select_dtypes(include=["object"]).columns:
            try:
                if df[col].astype(str).str.contains("ÔøΩ", na=False).any():
                    issues.append({
                        "type": "encoding_issue",
                        "severity": "medium",
                        "description": f"Kolumna '{col}' mo≈ºe mieƒá problem z kodowaniem",
                        "recommendation": "Sprawd≈∫ i wymu≈õ poprawne kodowanie pliku (UTF-8/CP1250 itd.)",
                    })
            except Exception:
                pass
        return issues

    def _generate_recommendations(self, df: pd.DataFrame) -> List[str]:
        recs: List[str] = []
        if len(df) > 0:
            miss_pct = (df.isnull().sum() / len(df)) * 100
            cols_with_missing = miss_pct[miss_pct > 0].index.tolist()
            if cols_with_missing:
                recs.append("üîç Zbadaj brakujƒÖce dane: " + ", ".join(cols_with_missing[:3]))

        num_cols = df.select_dtypes(include=[np.number]).columns
        if len(num_cols) > 1:
            rngs = df[num_cols].max() - df[num_cols].min()
            try:
                ratio = (rngs.replace(0, np.nan).max() / rngs.replace(0, np.nan).min())
                if np.isfinite(ratio) and ratio > 100:
                    recs.append("üìä Rozwa≈º standaryzacjƒô/normalizacjƒô ‚Äì bardzo r√≥≈ºne zakresy cech")
            except Exception:
                pass

        outliers = self._detect_outliers(df)
        high_out = [c for c, d in outliers.items() if d["iqr_outliers_percentage"] > 5]
        if high_out:
            recs.append("‚ö†Ô∏è Sprawd≈∫ outliers w kolumnach: " + ", ".join(high_out[:3]))

        corr = self._analyze_correlations(df)
        if corr.get("multicollinearity_warning"):
            recs.append("üîó Silne korelacje ‚Äì rozwa≈º redukcjƒô wymiarowo≈õci / usuniƒôcie redundantnych cech")

        cat_cols = df.select_dtypes(include=["object", "category"]).columns
        if len(cat_cols) > 0:
            recs.append("üõ†Ô∏è U≈ºyj encodowania zmiennych kategorycznych (one-hot, target/label encoding)")

        return recs

    # ====================== WIZUALIZACJE ======================

    def create_eda_visualizations(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Tworzy wykresy EDA ‚Äì bezpo≈õrednio do u≈ºycia w Streamlit (st.plotly_chart)."""
        visuals: Dict[str, Any] = {}

        # Braki
        missing = df.isnull().sum()
        if int(missing.sum()) > 0:
            m = missing[missing > 0]
            fig_missing = px.bar(
                x=m.index,
                y=m.values,
                title="üîç BrakujƒÖce dane w kolumnach",
                labels={"x": "Kolumny", "y": "Liczba brak√≥w"},
            )
            fig_missing.update_layout(height=400, margin=dict(l=50, r=50, t=80, b=50))
            visuals["missing_data"] = fig_missing

        # Korelacje
        num_cols = df.select_dtypes(include=[np.number]).columns
        if len(num_cols) > 1:
            corr = df[num_cols].corr()
            fig_corr = px.imshow(
                corr,
                text_auto=True,
                aspect="auto",
                title="üî• Macierz korelacji",
                color_continuous_scale="RdBu_r",
                zmin=-1, zmax=1,
            )
            fig_corr.update_layout(
                height=min(600, max(400, len(num_cols) * 40)),
                margin=dict(l=100, r=100, t=100, b=50),
                xaxis={"side": "bottom"},
            )
            visuals["correlation_matrix"] = fig_corr

        # Rozk≈Çady
        if len(num_cols) > 0:
            cols_to_plot = num_cols[:6]
            n_cols = min(3, len(cols_to_plot))
            n_rows = (len(cols_to_plot) + n_cols - 1) // n_cols

            fig_dist = make_subplots(
                rows=n_rows,
                cols=n_cols,
                subplot_titles=[f"Rozk≈Çad: {c}" for c in cols_to_plot],
                vertical_spacing=0.12,
                horizontal_spacing=0.1,
            )
            for i, c in enumerate(cols_to_plot):
                r = i // n_cols + 1
                cpos = i % n_cols + 1
                fig_dist.add_trace(
                    go.Histogram(x=df[c], name=c, showlegend=False, nbinsx=30),
                    row=r, col=cpos
                )
            fig_dist.update_layout(
                title_text="üìä Rozk≈Çady zmiennych numerycznych",
                height=max(400, 300 * n_rows),
                margin=dict(l=50, r=50, t=100, b=50),
                showlegend=False,
            )
            visuals["distributions"] = fig_dist

        return visuals

    # ====================== FEATURE IMPORTANCE ======================

    def get_feature_importance(
        self,
        estimator: Any,
        X: pd.DataFrame,
        y: pd.Series | np.ndarray,
        problem: str
    ) -> Dict[str, Any]:
        """Zwraca {'permutation': df?, 'shap': df?} je≈ºeli mo≈ºliwe."""
        out: Dict[str, Any] = {}
        try:
            perm = compute_permutation_importance(estimator, X, y, problem=problem)
            if perm is not None:
                out["permutation"] = perm
        except Exception:
            pass
        try:
            shap_imp = compute_shap_importance(estimator, X)
            if shap_imp is not None:
                out["shap"] = shap_imp
        except Exception:
            pass
        return out

    # ====================== Heurystyki pomocnicze ======================

    def _is_id_column(self, name: str, s: pd.Series) -> bool:
        keys = ["id", "index", "key", "pk", "primary"]
        n = name.lower()
        try:
            numeric_like = str(s.dtype).startswith("int") or str(s.dtype).startswith("uint")
            return (
                any(k in n for k in keys)
                or s.nunique(dropna=True) == len(s.dropna())
                or (numeric_like and s.min() >= 0 and s.nunique(dropna=True) > len(s.dropna()) * 0.9)
            )
        except Exception:
            return any(k in n for k in keys)

    def _is_year_like(self, name: str, s: pd.Series) -> bool:
        keys = ["year", "rok", "date", "time"]
        n = name.lower()
        try:
            numeric_like = str(s.dtype).startswith("int")
            s_nn = s.dropna()
            in_range = (
                numeric_like and len(s_nn) > 0
                and 1900 <= s_nn.min() <= 2035
                and 1900 <= s_nn.max() <= 2035
            )
            return any(k in n for k in keys) or in_range
        except Exception:
            return any(k in n for k in keys)

    def _is_percentage(self, s: pd.Series) -> bool:
        s = pd.to_numeric(s, errors="coerce")
        s = s.dropna()
        if s.empty:
            return False
        try:
            return (s.min() >= 0 and s.max() <= 1) or (s.min() >= 0 and s.max() <= 100)
        except Exception:
            return False

    def _is_binary_numeric(self, s: pd.Series) -> bool:
        u = set(pd.Series(s).dropna().unique().tolist())
        return u.issubset({0, 1}) or u.issubset({0.0, 1.0})

    def _is_name_column(self, name: str) -> bool:
        return any(k in name.lower() for k in ["name", "nazwa", "title", "tytu≈Ç", "label"])

    def _is_datetime_series(self, s: pd.Series) -> bool:
        try:
            sample = s.dropna().head(10)
            if sample.empty:
                return False
            pd.to_datetime(sample, errors="raise")
            return True
        except Exception:
            return False


# ======================================================================
#  Dodatkowe narzƒôdzia (je≈õli korzystasz z AI data-prep poza klasƒÖ)
# ======================================================================

def apply_ai_dataprep(df: pd.DataFrame, target: str, plan: dict) -> tuple[pd.DataFrame, List[Dict[str, Any]]]:
    """
    AI data-prep:
      - harmonizacja typ√≥w,
      - parsowanie i featuryzacja dat (rok/mies/dzie≈Ñ/dow/kwarta≈Ç itd.),
      - outliers IQR clip (bez targetu),
      - imputacja: num=mediana, kat='missing',
      - usuniƒôcie duplikat√≥w i kolumn sta≈Çych.
    Zwraca: (df_prepared, log_steps).
    """
    log: List[Dict[str, Any]] = []
    X = df.copy()
    log.append({"name": "Wczytanie", "detail": f"kszta≈Çt={X.shape}"})

    # category -> object
    cats = [c for c in X.columns if str(X[c].dtype).startswith("category")]
    if cats:
        for c in cats:
            X[c] = X[c].astype("object")
        log.append({"name": "Konwersja dtype", "detail": f"category‚Üíobject: {cats}"})

    # Parsowanie dat z obiekt√≥w
    obj_cols = X.select_dtypes(include=["object"]).columns.tolist()
    parsed: List[str] = []
    for c in obj_cols:
        ser = pd.to_datetime(X[c], errors="coerce", utc=False, infer_datetime_format=True)
        ok = ser.notna().mean() if len(ser) else 0.0
        if ok >= 0.8:
            X[c] = ser
            parsed.append(c)
    if parsed:
        log.append({"name": "Parsowanie dat", "detail": f"kolumny={parsed}"})

    # Ekstrakcja cech z datetime i drop orygina≈Ç√≥w
    dt_cols = X.select_dtypes(include=["datetime", "datetimetz", "datetime64[ns]", "datetime64[ns, UTC]"]).columns.tolist()
    made: List[str] = []
    for c in dt_cols:
        ser = pd.to_datetime(X[c], errors="coerce")
        X[f"{c}__year"] = ser.dt.year.astype("Int64")
        X[f"{c}__month"] = ser.dt.month.astype("Int64")
        X[f"{c}__day"] = ser.dt.day.astype("Int64")
        X[f"{c}__dayofweek"] = ser.dt.day_of_week.astype("Int64")
        X[f"{c}__quarter"] = ser.dt.quarter.astype("Int64")
        X[f"{c}__is_month_start"] = ser.dt.is_month_start.astype("Int64")
        X[f"{c}__is_month_end"] = ser.dt.is_month_end.astype("Int64")
        made.append(c)
    if made:
        X.drop(columns=made, inplace=True, errors="ignore")
        log.append({"name": "Ekstrakcja cech z dat", "detail": f"usuniƒôto orygina≈Çy: {made}"})

    # Duplikaty
    before = len(X)
    X = X.drop_duplicates()
    removed = before - len(X)
    if removed > 0:
        log.append({"name": "Usuniƒôto duplikaty wierszy", "detail": f"-{removed} wierszy"})

    # Sta≈Çe kolumny
    nunique = X.nunique(dropna=False)
    const_cols = nunique[nunique <= 1].index.tolist()
    if const_cols:
        X.drop(columns=const_cols, inplace=True, errors="ignore")
        log.append({"name": "Usuniƒôto kolumny sta≈Çe", "detail": f"kolumny={const_cols}"})

    # Outliers ‚Äì IQR clip (bez targetu)
    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()
    num_for_outliers = [c for c in num_cols if c != target]
    clipped: List[str] = []
    for c in num_for_outliers:
        s = X[c]
        if s.isna().all():
            continue
        q1, q3 = s.quantile(0.25), s.quantile(0.75)
        iqr = q3 - q1
        if not np.isfinite(iqr) or iqr == 0:
            continue
        low, high = q1 - 1.5 * iqr, q3 + 1.5 * iqr
        X[c] = s.clip(lower=low, upper=high)
        clipped.append(c)
    if clipped:
        log.append({"name": "Outliers IQR-clip", "detail": f"kolumny={clipped[:10]}{'...' if len(clipped)>10 else ''}"})

    # Imputacja
    for c in X.select_dtypes(include=[np.number]).columns:
        if X[c].isna().any():
            X[c] = X[c].fillna(X[c].median())
    for c in X.select_dtypes(include=["object", "category"]).columns:
        if X[c].isna().any():
            X[c] = X[c].fillna("missing")
    log.append({"name": "Imputacja brak√≥w", "detail": "num=mediana, kat='missing'"})

    log.append({"name": "Zako≈Ñczono data-prep", "detail": f"kszta≈Çt={X.shape}"})
    return X, log
